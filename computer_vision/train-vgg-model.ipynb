{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FaRwn-tcQN8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "#import skvideo.io\n",
        "import tqdm\n",
        "import sys\n",
        "import numpy as np\n",
        "import csv\n",
        "import cv2\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFjeaOL2fSND",
        "outputId": "97cdd13a-1b67-44eb-ddee-e542e4c50e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9_VBPe8cQN8"
      },
      "outputs": [],
      "source": [
        "LENGTH=10 #in secs\n",
        "FRAME_RATE=24 # Frame rate of the frame videos\n",
        "SAMPLE_RATE=2 # Sampling rate for feature extraction\n",
        "NUM_FRAMES=int(LENGTH*FRAME_RATE/SAMPLE_RATE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgq4I2YbcQN8"
      },
      "outputs": [],
      "source": [
        "# USER INPUT- data_dir and features_dir\n",
        "# data_dir- path to frame videos. Required only for feature extraction\n",
        "# features_dir- path to save or load vgg face features\n",
        "\n",
        "DIF_PATH = Path.cwd() / 'gdrive' / 'MyDrive' / 'DIF'\n",
        "DIF_PATH_STR = str(DIF_PATH)\n",
        "#data_dir=repo_path+'/DIFv2'+'/'+str(LENGTH)+'/frame_video' # USER INPUT Path to frame videos directory. Required only for feature extraction\n",
        "features_dir = DIF_PATH / 'vgg_face7' # USER INPUT Path to save/load vgg face features\n",
        "features_dir_str = str(features_dir)\n",
        "#features_path=repo_path+'/features' # Folder containing all features audio and video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiMk1uQrcQN-"
      },
      "source": [
        "# Data generator\n",
        "\n",
        "Link-https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VyHkqukrcQN-"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Input-  csv_file\n",
        "Output- partition train, val test. Each partition consists of list of .npy files and dictionary of labels.\n",
        "'''\n",
        "def train_test_split(csv_path):\n",
        "    label={'Drunk':1, 'Sober':0}\n",
        "    partition={}\n",
        "    train={}\n",
        "    val={}\n",
        "    test={}\n",
        "\n",
        "    train_list=[]\n",
        "    val_list=[]\n",
        "    test_list=[]\n",
        "    train_label={}\n",
        "    val_label={}\n",
        "    test_label={}\n",
        "\n",
        "    with open(csv_path) as csvfile:\n",
        "        reader=csv.reader(csvfile,delimiter=',')\n",
        "        for row in reader:\n",
        "            filename=row[2]\n",
        "            filename=filename[:-4]\n",
        "            if row[0]=='train':\n",
        "                train_label[filename]=label[row[1]]\n",
        "                train_list.append(filename)\n",
        "            elif row[0]=='val':\n",
        "                val_label[filename]=label[row[1]]\n",
        "                val_list.append(filename)\n",
        "            elif row[0]=='test':\n",
        "                test_label[filename]=label[row[1]]\n",
        "                test_list.append(filename)\n",
        "            else:\n",
        "                print(\"Error in label\")\n",
        "                return None\n",
        "    train['list']=train_list\n",
        "    val['list']=val_list\n",
        "    test['list']=test_list\n",
        "\n",
        "    train['label']=train_label\n",
        "    val['label']=val_label\n",
        "    test['label']=test_label\n",
        "\n",
        "    partition['train']=train\n",
        "    partition['val']=val\n",
        "    partition['test']=test\n",
        "\n",
        "    return partition\n",
        "def count_classes(d):\n",
        "    values=list(d.values())\n",
        "    zeros=values.count(0)\n",
        "    return (zeros,len(values)-zeros)\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, list_IDs, labels, datapath, batch_size=32, dim=(24,1000),n_classes=2, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        #self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.path=datapath\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.list_IDs) / self.batch_size))\n",
        "\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.list_IDs))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generate one batch of data'\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of IDs\n",
        "        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        X, y = self.__data_generation(list_IDs_temp)\n",
        "        import gc\n",
        "        gc.collect()\n",
        "        return X, y\n",
        "\n",
        "    def __data_generation(self, list_IDs_temp):\n",
        "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
        "        # Initialization\n",
        "        X = np.empty((self.batch_size, *self.dim))\n",
        "        y = np.empty((self.batch_size), dtype=int)\n",
        "\n",
        "        # Generate data\n",
        "        for i, ID in enumerate(list_IDs_temp):\n",
        "            # Store sample\n",
        "            X[i,] = np.load(self.path+'/' + ID + '.npy')\n",
        "\n",
        "            # Store class\n",
        "            y[i] = self.labels[ID]\n",
        "\n",
        "        return X, keras.utils.to_categorical(y, num_classes=self.n_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASj2SGMYcQN-"
      },
      "source": [
        "# 3 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_D3rcwSjcQN-"
      },
      "source": [
        "\n",
        "## 3.1 Model creation and summary\n",
        "Batch normaliztion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMB5LMSccQN-"
      },
      "outputs": [],
      "source": [
        "from keras.optimizers import Adam\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from keras.layers import Dense, Input, Dropout, LSTM, Activation,BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import TensorBoard\n",
        "from time import time\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7urXkGugcQN-"
      },
      "outputs": [],
      "source": [
        "def create_model(num_class,lstm_units,dropout,input_shape=(None,1000)):\n",
        "    X=Input(shape=input_shape)\n",
        "    norm=BatchNormalization()(X)\n",
        "    feat=LSTM(units=lstm_units)(norm)\n",
        "    drop=Dropout(rate=dropout)(feat)\n",
        "    prob=Dense(num_class, activation='sigmoid')(drop)\n",
        "    return Model(inputs = X, outputs = prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJqmOblEcQN-",
        "outputId": "d6743513-d501-4086-d9eb-b22fc27456a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, 120, 4096)]       0         \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 120, 4096)         16384     \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 128)               2163200   \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2179842 (8.32 MB)\n",
            "Trainable params: 2171650 (8.28 MB)\n",
            "Non-trainable params: 8192 (32.00 KB)\n",
            "_________________________________________________________________\n",
            "Must check the csv path...........\n"
          ]
        }
      ],
      "source": [
        "lstm_units=128\n",
        "dropout=.2\n",
        "hp=3\n",
        "model=create_model(2,lstm_units,dropout,(NUM_FRAMES,4096))\n",
        "model_path = DIF_PATH_STR+'/model' # USER INPUT, path to save/load model\n",
        "model.summary()\n",
        "print(\"Must check the csv path...........\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaiYJ-FCcQN-"
      },
      "source": [
        "## 3.2 Fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "siQmNdSvcQN-"
      },
      "outputs": [],
      "source": [
        "#USER INPUT\n",
        "#split_path = repo_path+'/DIFv2/'+str(LENGTH)+'/train_test_sets/1/split_4540_642_948.csv'# or enter path to the split.csv in the parent directory\n",
        "split_path = DIF_PATH_STR+'/split_4550_642_940.csv'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2ZmvAWxcQN-",
        "outputId": "15a5a6f5-fc33-45e5-9108-849427ce8475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training examples \n",
            "4542\n",
            "Number of validation examples \n",
            "640\n",
            "Class instances in training class.\n",
            " Sober: 1000  Drunk: 3542\n",
            "Class instances in val class.\n",
            " Sober: 319  Drunk: 321\n",
            "Class instances in test class.\n",
            " Sober: 356  Drunk: 584\n"
          ]
        }
      ],
      "source": [
        "def load_keras_model(path):\n",
        "    if os.path.isfile(path):\n",
        "        return load_model(path)\n",
        "#Loading data filenames split\n",
        "\n",
        "partition=train_test_split(split_path)\n",
        "print(\"Number of training examples \")\n",
        "print(len(partition['train']['list']))\n",
        "print(\"Number of validation examples \")\n",
        "print(len(partition['val']['list']))\n",
        "\n",
        "params = {'datapath':features_dir_str ,\n",
        "          'dim': (NUM_FRAMES,4096),\n",
        "          'batch_size': 64,\n",
        "          'n_classes': 2,\n",
        "          'shuffle': False}\n",
        "\n",
        "#weights for imbalance classes\n",
        "count=count_classes(partition['train']['label'])\n",
        "print(\"Class instances in training class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n",
        "weight_0=float(count[0]+count[1])/float(count[0])\n",
        "weight_1=float(count[0]+count[1])/float(count[1])\n",
        "class_weight={0:weight_0, 1:weight_1}\n",
        "\n",
        "#instances in val set\n",
        "count=count_classes(partition['val']['label'])\n",
        "print(\"Class instances in val class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n",
        "\n",
        "#instances in test set\n",
        "count=count_classes(partition['test']['label'])\n",
        "print(\"Class instances in test class.\\n Sober:\",count[0],\" Drunk:\",count[1])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mA6EVg-UcQN_",
        "outputId": "67303d3e-72aa-40f1-f9c6-a9b961442474",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "generator created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-75-bffae7323ff3>:15: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  model.fit_generator(generator=train_generator,\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "70/70 [==============================] - 760s 4s/step - loss: 1.3903 - accuracy: 0.6237 - val_loss: 0.5625 - val_accuracy: 0.7125\n",
            "Epoch 2/10\n",
            "70/70 [==============================] - 222s 3s/step - loss: 0.7118 - accuracy: 0.8855 - val_loss: 0.6012 - val_accuracy: 0.7047\n",
            "Epoch 3/10\n",
            "70/70 [==============================] - 237s 3s/step - loss: 0.2366 - accuracy: 0.9848 - val_loss: 0.5170 - val_accuracy: 0.7781\n",
            "Epoch 4/10\n",
            "70/70 [==============================] - 228s 3s/step - loss: 0.0902 - accuracy: 0.9933 - val_loss: 0.5887 - val_accuracy: 0.7656\n",
            "Epoch 5/10\n",
            "70/70 [==============================] - 218s 3s/step - loss: 0.9855 - accuracy: 0.7830 - val_loss: 0.7217 - val_accuracy: 0.6875\n",
            "Epoch 6/10\n",
            "70/70 [==============================] - 238s 3s/step - loss: 0.5893 - accuracy: 0.9011 - val_loss: 0.6645 - val_accuracy: 0.7344\n",
            "Epoch 7/10\n",
            "70/70 [==============================] - 238s 3s/step - loss: 0.3323 - accuracy: 0.9746 - val_loss: 0.6711 - val_accuracy: 0.7484\n",
            "Epoch 8/10\n",
            "70/70 [==============================] - 238s 3s/step - loss: 0.2001 - accuracy: 0.9920 - val_loss: 0.7308 - val_accuracy: 0.7047\n",
            "Epoch 9/10\n",
            "70/70 [==============================] - 240s 3s/step - loss: 0.1250 - accuracy: 0.9958 - val_loss: 0.7915 - val_accuracy: 0.7016\n",
            "Epoch 10/10\n",
            "70/70 [==============================] - 239s 3s/step - loss: 0.0904 - accuracy: 0.9969 - val_loss: 0.8102 - val_accuracy: 0.7078\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7cd6203cfd90>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
        "\n",
        "#saving best model\n",
        "checkpoint = ModelCheckpoint(model_path+'/model-{epoch:03d}-{val_acc:03f}.h5', verbose=1, monitor='val_acc',save_best_only=False, mode='max',period=10)\n",
        "\n",
        "\n",
        "#tensorboard\n",
        "tensorboard = TensorBoard(log_dir=model_path)\n",
        "\n",
        "train_generator=DataGenerator(partition['train']['list'],partition['train']['label'], **params)\n",
        "val_generator=DataGenerator(partition['val']['list'],partition['val']['label'], **params)\n",
        "\n",
        "print(\"generator created\")\n",
        "\n",
        "model.fit_generator(generator=train_generator,\n",
        "                    epochs=10,\n",
        "                    validation_data=val_generator,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=4,\n",
        "                    #callbacks=[checkpoint,tensorboard],\n",
        "                    class_weight=class_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PIdcZX3HIBh"
      },
      "outputs": [],
      "source": [
        "model.save(model_path+'/model-{epoch:03d}-{val_acc:03f}.keras')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
